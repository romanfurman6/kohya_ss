from lora_gui import train_model

if __name__ == '__main__':
    train_model(
        headless={ "label": False },
        print_only={ "label": False },
        pretrained_model_name_or_path="/workspace/models/HassanBlend1.5.1.2.ckpt",
        v2=False,
        v_parameterization=False,
        sdxl=False,
        logging_dir="/workspace/result_jj_hassan/log",
        train_data_dir="/workspace/result_jj_hassan/img",
        reg_data_dir="/workspace/result_jj_hassan/reg",
        output_dir="/workspace/result_jj_hassan/model",
        max_resolution=768,
        learning_rate=0.0001,
        lr_scheduler="cosine",
        lr_warmup=10, 
        train_batch_size=2,
        epoch=20,
        save_every_n_epochs=4,
        mixed_precision="bf16",
        save_precision="bf16",
        seed=0,
        num_cpu_threads_per_process=2,
        cache_latents=True,
        cache_latents_to_disk=False,
        caption_extension="",
        enable_bucket=True,
        gradient_checkpointing="bucket",
        full_fp16=False,
        no_token_padding=False,
        stop_text_encoder_training_pct=0,
        xformers=True,
        save_model_as="ckpt",
        shuffle_caption=False,
        save_state=False,
        resume=None,
        prior_loss_weight=1.0,
        text_encoder_lr=0.00005,
        unet_lr=0.0001,
        network_dim=128,
        lora_network_weights="",
        dim_from_weights=False,
        color_aug=False,
        flip_aug=False,
        clip_skip=1,
        gradient_accumulation_steps=1,
        mem_eff_attn=False,
        output_name="latest",
        model_list="custom",
        max_token_length='75',
        max_train_epochs=None,
        max_data_loader_n_workers='0',
        network_alpha=1,
        training_comment="",
        keep_tokens=0,
        lr_scheduler_num_cycles="",
        lr_scheduler_power="",
        persistent_data_loader_workers=False,
        bucket_no_upscale=True,
        random_crop=False,
        bucket_reso_steps=64,
        caption_dropout_every_n_epochs=0,
        caption_dropout_rate=0,
        optimizer="AdamW8bit",
        optimizer_args="",
        noise_offset_type='Original',
        noise_offset=0,
        adaptive_noise_scale=0,
        multires_noise_iterations=0,
        multires_noise_discount=0,
        LoRA_type='Standard',
        factor=None,
        use_cp=False,
        decompose_both=False,
        train_on_input=False,
        conv_dim=None,
        conv_alpha=None,
        sample_every_n_steps=0,
        sample_every_n_epochs=0,
        sample_sampler="k_euler_a",
        sample_prompts="",
        additional_parameters=None,
        vae_batch_size=0,
        min_snr_gamma=0,
        down_lr_weight=None,
        mid_lr_weight=None,
        up_lr_weight=None,
        block_lr_zero_threshold=None,
        block_dims=None,
        block_alphas=None,
        conv_dims=None,
        conv_alphas=None,
        weighted_captions=False,
        unit=1, # todo
        save_every_n_steps=0,
        save_last_n_steps=0,
        save_last_n_steps_state=0,
        use_wandb=False,
        wandb_api_key="",
        scale_v_pred_loss_like_noise_pred=False,
        scale_weight_norms=0,
        network_dropout=0,
        rank_dropout=0,
        module_dropout=0,
        sdxl_cache_text_encoder_outputs=False,
        sdxl_no_half_vae=False,
        min_timestep=0,
        max_timestep=1000
    )
